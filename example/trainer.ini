[Project]
name=LLM
version=1.0
user=garfield
pipeline=Trainer
proxies={"http": "127.0.0.1:7890", "https": "127.0.0.1:7890"}


[TokenizerLoader]
pretrained_model_name_or_path=Writer/palmyra-small
use_fast=True
split_special_tokens=True
padding_side=right
device_map=auto
trust_remote_code=True


[AutoConfig]
pretrained_model_name_or_path=Writer/palmyra-small
model_type=qwen
compute_dtype=bf16
trust_remote_code=True


is_trainable=True

# for Qwen models
use_dynamic_ntk=True
use_logn_attn=True

# for LLama and Falcon models
rope_scaling=dynamic
scaling_factor=1.0

flash_attn=True
shift_attn=True



[ModelLoader]
class=transformers.AutoModelForCausalLM
print_model_structure=False
pretrained_model_name_or_path=Writer/palmyra-small

compute_dtype=bf16
quantization_bit=8


[LoraConfig]
task_type=CAUSAL_LM
inference_mode=False
r=16
lora_alpha=32
lora_dropout=0.05
target_modules=c_fc
bias=lora_only
fan_in_fan_out=True


[AdapterLoader]
# type should be full or lora
type=lora
model=ModelLoader
config_checkpoint_dir=
lora_config=LoraConfig


[TrainingArguments]
#class=transformers.TrainingArguments
class=transformers.Seq2SeqTrainingArguments
generation_max_length=128
generation_num_beams=2

per_device_train_batch_size=1
per_device_eval_batch_size=1
gradient_accumulation_steps=2
warmup_steps=20
max_steps=40
num_train_epochs=1
learning_rate=2e-4
#fp16=True
logging_steps=25
output_dir=outputs
do_train=True
remove_unused_columns=False

[DataCollator]
class=transformers.DataCollatorForLanguageModeling
tokenizer=TokenizerLoader
mlm=False
#class=transformers.DataCollatorForSeq2Seq
#tokenizer=TokenizerLoader
#pad_to_multiple_of=4
#label_pad_token_id=-100


[Trainer]
class=transformers.Seq2SeqTrainer
model=ModelLoader
tokenizer=TokenizerLoader
args=TrainingArguments
data_collator=DataCollator
callbacks=callbacks
dataset=DatasetProcess
stage=train
resume_from_checkpoint=
plot_loss=True
output_dir=../output

streaming=False
split_train_val=True
split_train_val_val_size=20
split_train_val_seed=2024
split_train_val_buffer_size=10


[DatasetLoader_1]
# type should be hf_hub or script or file
path=json
data_files=../data/oaast_sft_zh.json
split=train
cache_dir=../cache
streaming=False
use_auth_token=False


[DatasetProcess]
dataset=DatasetLoader_1
tokenizer=TokenizerLoader
# stage can be pt or sft or rm or ppo
stage=pt
cutoff_len=128

