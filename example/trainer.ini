[Project]
name=LLM
version=1.0
user=garfield
pipeline=Trainer
#proxies={"http": "127.0.0.1:7890", "https": "127.0.0.1:7890"}

[Lora]
r=16
lora_alpha=32
target_modules=q_proj,v_proj
lora_dropout=0.05
bias=none,
task_type=CAUSAL_LM


[TokenizerLoader]
pretrained_model_name_or_path=THUDM/chatglm3-6b
use_fast=True
split_special_tokens=True
padding_side=right
device_map=auto
trust_remote_code=True


[ModelLoader]
pretrained_model_name_or_path=facebook/opt-1.3b


[TrainingArguments]
per_device_train_batch_size=4
gradient_accumulation_steps=4
warmup_steps=100
max_steps=200
learning_rate=2e-4
#fp16=True
logging_steps=25
output_dir=outputs


[DataCollator]
class=transformers.DataCollatorForLanguageModeling
tokenizer=TokenizerLoader
mlm=False


[Trainer]
model=ModelLoader
tokenizer=TokenizerLoader
args=TrainingArguments
data_collator=DataCollator
callbacks=callbacks
dataset=DatasetLoader_1
stage=train
resume_from_checkpoint=
plot_loss=True
output_dir=


[DatasetLoader_1]
# type should be hf_hub or script or file
type=file
path=../data
value=../data/self_cognition.json
split=train
cache_dir=
streaming=False
use_auth_token=False
split_train_val=True
split_train_val_val_size=20
split_train_val_seed=2024
split_train_val_buffer_size=10



