[Project]
name=LLM
version=1.0
user=garfield
#proxies={"http": "127.0.0.1:7890", "https": "127.0.0.1:7890"}
checkpoint_dir=checkpoint
pipeline=Trainer
# stage can be pt,sft,ppo or rm
stage=ppo

[TokenizerLoader]
pretrained_model_name_or_path=gpt2
use_fast=True
split_special_tokens=True
padding_side=right
device_map=auto
trust_remote_code=True

[ModelLoader]
class=transformers.AutoModelForCausalLM
print_model_structure=False
pretrained_model_name_or_path=gpt2
trust_remote_code=True
cache_dir=./cache
revision=
use_auth_token=False
#torch_dtype=bf16
is_trainable=True

# for LLama and Falcon models
rope_scaling=dynamic
model_max_length=2000

flash_attn=False
shift_attn=False
#quantization_bit=8
#double_quantization=4
quantization_type=
reward_model=../output_rm

config_checkpoint_dir=
# type should be full or lora
type=lora
lora_config_task_type=CAUSAL_LM
lora_config_inference_mode=False
lora_config_r=16
lora_config_lora_alpha=32
lora_config_lora_dropout=0.05
lora_config_target_modules=c_fc
# bias can be set none or all or lora_only
lora_config_bias=none
lora_config_fan_in_fan_out=True

[TrainingArguments]
#class=transformers.TrainingArguments
class=transformers.Seq2SeqTrainingArguments
generation_max_length=256

generation_num_beams=1
per_device_train_batch_size=1
per_device_eval_batch_size=1
gradient_accumulation_steps=2
warmup_steps=20
max_steps=40
num_train_epochs=1
learning_rate=2e-4
#fp16=True
logging_steps=25
do_train=True
remove_unused_columns=False
output_dir=../output_ppo
save_safetensors=False
seed=2023

[PPOArguments]
# Target KL value for adaptive KL control in PPO training.
target=6.0
log_with=
# Use score normalization in PPO training.
use_score_scaling=False
# Use score normalization in PPO training.
use_score_norm=False
accelerator_kwargs={"step_scheduler_with_optimizer": false}
optimize_cuda_cache=True

[Trainer]
class=transformers.Seq2SeqTrainer
model=ModelLoader
tokenizer=TokenizerLoader
args=TrainingArguments
callbacks=callbacks
dataset=DatasetLoader_1
resume_from_checkpoint=False
plot_loss=True
ignore_pad_token_for_loss=True
streaming=False
split_train_val=True
split_train_val_val_size=20
split_train_val_seed=2024
split_train_val_buffer_size=10
steps=train,eval,predict
predict_with_generate=True
ppo_args=PPOArguments


[DatasetLoader_1]
# type should be hf_hub or script or file
path=json
data_files=../data/oaast_rm_zh.json
text_column=instruction
prompt_column=instruction
query_column=input
history_column=history
response_column=output
system_column=system
split=train
cache_dir=../cache
streaming=False
use_auth_token=False

# etl process params
tokenizer=TokenizerLoader
# stage can be pt or sft or rm or ppo
cutoff_len=128
sft_packing=True
render=vanilla
label_mask_prompt=True