[Project]
name=LLM
version=1.0
user=garfield
pipeline=Trainer
proxies={"http": "127.0.0.1:7890", "https": "127.0.0.1:7890"}

[TokenizerLoader]
pretrained_model_name_or_path=Writer/palmyra-small
use_fast=True
split_special_tokens=True
padding_side=right
device_map=auto
trust_remote_code=True


[ModelLoader]
pretrained_model_name_or_path=Writer/palmyra-small


[LoraConfig]
task_type=CAUSAL_LM
inference_mode=False
r=16
lora_alpha=32
lora_dropout=0.05
target_modules=c_fc
bias=None
fan_in_fan_out=True


[AdapterLoader]
# type should be full or lora
type=lora
model=ModelLoader
config_checkpoint_dir=
lora_config=LoraConfig


[TrainingArguments]
per_device_train_batch_size=1
per_device_eval_batch_size=1
gradient_accumulation_steps=2
warmup_steps=20
# max_steps=40
num_train_epochs=1
learning_rate=2e-4
#fp16=True
logging_steps=25
output_dir=outputs
do_train=True


[DataCollator]
class=transformers.DataCollatorForLanguageModeling
tokenizer=TokenizerLoader
mlm=False


[Trainer]
model=AdapterLoader
tokenizer=TokenizerLoader
args=TrainingArguments
data_collator=DataCollator
callbacks=callbacks
dataset=DatasetLoader_1
stage=train
resume_from_checkpoint=
plot_loss=True
output_dir=


[DatasetLoader_1]
# type should be hf_hub or script or file
path=json
data_files=../data/oaast_sft_zh.json
split=train
cache_dir=../cache
streaming=False
use_auth_token=False
split_train_val=True
split_train_val_val_size=20
split_train_val_seed=2024
split_train_val_buffer_size=10



